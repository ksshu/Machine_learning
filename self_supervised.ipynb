{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled51.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPnsj4RRXCSr4XlyXee9RJt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ksshu/Machine_learning/blob/main/self_supervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "7ysOidewDAxH",
        "outputId": "7d40a8ce-9041-423e-a3c2-c0de022b3adc"
      },
      "source": [
        "# Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
        "# 4.0 International License. To view a copy of this license, visit\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
        "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import imageio\n",
        "\n",
        "import h5py\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import PIL.Image\n",
        "import math\n",
        "import glob\n",
        "import pickle\n",
        "import re\n",
        "\n",
        "import dnnlib\n",
        "import dnnlib.tflib\n",
        "import dnnlib.tflib.tfutil as tfutil\n",
        "from dnnlib.tflib.autosummary import autosummary\n",
        "import dnnlib.submission.submit as submit\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Misc helpers.\n",
        "\n",
        "def init_tf(seed=None):\n",
        "    config_dict = {'graph_options.place_pruned_graph': True, 'gpu_options.allow_growth': True}\n",
        "    if tf.get_default_session() is None:\n",
        "        tf.set_random_seed(np.random.randint(1 << 31) if (seed is None) else seed)\n",
        "        tfutil.create_session(config_dict, force_as_default=True)\n",
        "\n",
        "def adjust_dynamic_range(data, drange_in, drange_out):\n",
        "    if drange_in != drange_out:\n",
        "        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (np.float32(drange_in[1]) - np.float32(drange_in[0]))\n",
        "        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n",
        "        data = data * scale + bias\n",
        "    return data\n",
        "\n",
        "def convert_to_pil_image(image, drange=[0,1]):\n",
        "    assert image.ndim == 2 or image.ndim == 3\n",
        "    if image.ndim == 3:\n",
        "        if image.shape[0] == 1:\n",
        "            image = image[0] # grayscale CHW => HW\n",
        "        else:\n",
        "            image = image.transpose(1, 2, 0) # CHW -> HWC\n",
        "\n",
        "    image = adjust_dynamic_range(image, drange, [0, 255])\n",
        "    image = np.rint(image).clip(0, 255).astype(np.uint8)\n",
        "    fmt = 'RGB' if image.ndim == 3 else 'L'\n",
        "    return PIL.Image.fromarray(image, fmt)\n",
        "\n",
        "def save_image(image, filename, drange=[0,1], quality=95):\n",
        "    img = convert_to_pil_image(image, drange)\n",
        "    if '.jpg' in filename:\n",
        "        img.save(filename,\"JPEG\", quality=quality, optimize=True)\n",
        "    else:\n",
        "        img.save(filename)\n",
        "\n",
        "def save_snapshot(submit_config, net, fname_postfix):\n",
        "    dump_fname = os.path.join(submit_config.run_dir, \"network-%s.pickle\" % fname_postfix)\n",
        "    with open(dump_fname, \"wb\") as f:\n",
        "        pickle.dump(net, f)\n",
        "\n",
        "def compute_ramped_lrate(i, iteration_count, ramp_up_fraction, ramp_down_fraction, learning_rate):\n",
        "    if ramp_up_fraction > 0.0:\n",
        "        ramp_up_end_iter = iteration_count * ramp_up_fraction\n",
        "        if i <= ramp_up_end_iter:\n",
        "            t = (i / ramp_up_fraction) / iteration_count\n",
        "            learning_rate = learning_rate * (0.5 - np.cos(t * np.pi)/2)\n",
        "\n",
        "    if ramp_down_fraction > 0.0:\n",
        "        ramp_down_start_iter = iteration_count * (1 - ramp_down_fraction)\n",
        "        if i >= ramp_down_start_iter:\n",
        "            t = ((i - ramp_down_start_iter) / ramp_down_fraction) / iteration_count\n",
        "            learning_rate = learning_rate * (0.5 + np.cos(t * np.pi)/2)**2\n",
        "\n",
        "    return learning_rate\n",
        "\n",
        "def clip_to_uint8(arr):\n",
        "    if isinstance(arr, np.ndarray):\n",
        "        return np.clip(arr * 255.0 + 0.5, 0, 255).astype(np.uint8)\n",
        "    x = tf.clip_by_value(arr * 255.0 + 0.5, 0, 255)\n",
        "    return tf.cast(x, tf.uint8)\n",
        "\n",
        "def calculate_psnr(a, b, axis=None):\n",
        "    a, b = [clip_to_uint8(x) for x in [a, b]]\n",
        "    if isinstance(a, np.ndarray):\n",
        "        a, b = [x.astype(np.float32) for x in [a, b]]\n",
        "        x = np.mean((a - b)**2, axis=axis)\n",
        "        return np.log10((255 * 255) / x) * 10.0\n",
        "    a, b = [tf.cast(x, tf.float32) for x in [a, b]]\n",
        "    x = tf.reduce_mean((a - b)**2, axis=axis)\n",
        "    return tf.log((255 * 255) / x) * (10.0 / math.log(10))\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def poisson(x, lam):\n",
        "    if lam > 0.0:\n",
        "        return np.random.poisson(x * lam) / lam\n",
        "    return 0.0 * x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "# Number of channels enforcer while retaining dtype.\n",
        "def set_color_channels(x, num_channels):\n",
        "    assert x.shape[0] in [1, 3, 4]\n",
        "    x = x[:min(x.shape[0], 3)] # drop possible alpha channel\n",
        "    if x.shape[0] == num_channels:\n",
        "        return x\n",
        "    elif x.shape[0] == 1:\n",
        "        return np.tile(x, [3, 1, 1])\n",
        "    y = np.mean(x, axis=0, keepdims=True)\n",
        "    if np.issubdtype(x.dtype, np.integer):\n",
        "        y = np.round(y).astype(x.dtype)\n",
        "    return y\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def load_datasets(num_channels, dataset_dir, train_dataset, validation_dataset, prune_dataset=None):\n",
        "    # Training set.\n",
        "\n",
        "    if train_dataset is None:\n",
        "        print(\"Not loading training data.\")\n",
        "        train_images = []\n",
        "    else:\n",
        "        fn = submit.get_path_from_template(train_dataset)\n",
        "        print(\"Loading training dataset from '%s'.\" % fn)\n",
        "\n",
        "        h5file = h5py.File(fn, 'r')\n",
        "        num = h5file['images'].shape[0]\n",
        "        print(\"Dataset contains %d images.\" % num)\n",
        "\n",
        "        if prune_dataset is not None:\n",
        "            num = prune_dataset\n",
        "            print(\"Pruned down to %d first images.\" % num)\n",
        "\n",
        "        # Load the images.\n",
        "        train_images = [None] * num\n",
        "        bs = 1024\n",
        "        for i in range(0, num, bs):\n",
        "            sys.stdout.write(\"\\r%d / %d ..\" % (i, num))\n",
        "            n = min(bs, num - i)\n",
        "            img = h5file['images'][i : i+n]\n",
        "            shp = h5file['shapes'][i : i+n]\n",
        "            for j in range(n):\n",
        "                train_images[i+j] = set_color_channels(np.reshape(img[j], shp[j]), num_channels)\n",
        "\n",
        "        print(\"\\nLoading done.\")\n",
        "        h5file.close()\n",
        "\n",
        "    if validation_dataset in ['kodak', 'bsd300', 'set14']:\n",
        "        paths = { 'kodak':  os.path.join(dataset_dir, 'kodak', '*.png'),\n",
        "                  'bsd300': os.path.join(dataset_dir, 'BSDS300', 'images/test/*.jpg'),   # Just the 100 test images\n",
        "                  'set14':  os.path.join(dataset_dir, 'Set14', '*.png')}\n",
        "        fn = submit.get_path_from_template(paths[validation_dataset])\n",
        "        print(\"Loading validation dataset from '%s'.\" % fn)\n",
        "        validation_images = [imageio.imread(x, ignoregamma=True) for x in glob.glob(fn)]\n",
        "        validation_images = [x[..., np.newaxis] if len(x.shape) == 2 else x for x in validation_images] # Add channel axis to grayscale images.\n",
        "        validation_images = [x.transpose([2, 0, 1]) for x in validation_images]\n",
        "        validation_images = [set_color_channels(x, num_channels) for x in validation_images] # Enforce RGB/grayscale mode.\n",
        "        print(\"Loaded %d images.\" % len(validation_images))\n",
        "\n",
        "    # Pad the validation images to size.\n",
        "    validation_image_size = [max([x.shape[axis] for x in validation_images]) for axis in [1, 2]]\n",
        "    validation_image_size = [(x + 31) // 32 * 32 for x in validation_image_size] # Round up to a multiple of 32.\n",
        "    validation_image_size = [max(validation_image_size) for x in validation_image_size] # Square it up for the rotators.\n",
        "    print(\"Validation image padded size = [%d, %d].\" % (validation_image_size[0], validation_image_size[1]))\n",
        "\n",
        "    return train_images, validation_images, validation_image_size\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Backbone autoencoder network, optional blind spot.\n",
        "\n",
        "def analysis_network(image, num_output_components, blindspot, zero_last=False):\n",
        "\n",
        "    def conv(n, name, n_out, size=3, gain=np.sqrt(2), zero_weights=False):\n",
        "        if blindspot: assert (size % 2) == 1\n",
        "        ofs = 0 if (not blindspot) else size // 2\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            wshape = [size, size, n.shape[1].value, n_out]\n",
        "            wstd = gain / np.sqrt(np.prod(wshape[:-1])) # He init.\n",
        "            W = tf.get_variable('W', shape=wshape, initializer=(tf.initializers.zeros() if zero_weights else tf.initializers.random_normal(0., wstd)))\n",
        "            b = tf.get_variable('b', shape=[n_out], initializer=tf.initializers.zeros())\n",
        "            if ofs > 0: n = tf.pad(n, [[0, 0], [0, 0], [ofs, 0], [0, 0]])\n",
        "            n = tf.nn.conv2d(n, W, strides=[1]*4, padding='SAME', data_format='NCHW') + tf.reshape(b, [1, -1, 1, 1])\n",
        "            if ofs > 0: n = n[:, :, :-ofs, :]\n",
        "        return n\n",
        "\n",
        "    def up(n, name):\n",
        "        with tf.name_scope(name):\n",
        "            s = tf.shape(n)\n",
        "            s = [-1, n.shape[1], s[2], s[3]]\n",
        "            n = tf.reshape(n, [s[0], s[1], s[2], 1, s[3], 1])\n",
        "            n = tf.tile(n, [1, 1, 1, 2, 1, 2])\n",
        "            n = tf.reshape(n, [s[0], s[1], s[2] * 2, s[3] * 2])\n",
        "        return n\n",
        "\n",
        "    def down(n, name):\n",
        "        with tf.name_scope(name):\n",
        "            if blindspot: # Shift and pad if blindspot.\n",
        "                n = tf.pad(n[:, :, :-1, :], [[0, 0], [0, 0], [1, 0], [0, 0]])\n",
        "            n = tf.nn.max_pool(n, ksize=[1, 1, 2, 2], strides=[1, 1, 2, 2], padding='SAME', data_format='NCHW')\n",
        "        return n\n",
        "\n",
        "    def rotate(x, angle):\n",
        "        if   angle == 0:   return x\n",
        "        elif angle == 90:  return tf.transpose(x[:, :, :, ::-1], [0, 1, 3, 2])\n",
        "        elif angle == 180: return x[:, :, ::-1, ::-1]\n",
        "        elif angle == 270: return tf.transpose(x[:, :, ::-1, :], [0, 1, 3, 2])\n",
        "\n",
        "    def concat(name, layers):\n",
        "        return tf.concat(layers, axis=1, name=name)\n",
        "\n",
        "    def LR(n, alpha=0.1):\n",
        "        return tf.nn.leaky_relu(n, alpha=alpha, name='lrelu')\n",
        "\n",
        "    # Input stage.\n",
        "\n",
        "    if not blindspot:\n",
        "        x = image\n",
        "    else:\n",
        "        x = tf.concat([rotate(image, a) for a in [0, 90, 180, 270]], axis=0)\n",
        "\n",
        "    # Encoder part.\n",
        "\n",
        "    pool0 = x\n",
        "    x = LR(conv(x, 'enc_conv0', 48))\n",
        "    x = LR(conv(x, 'enc_conv1', 48))\n",
        "    x = down(x, 'pool1'); pool1 = x\n",
        "\n",
        "    x = LR(conv(x, 'enc_conv2', 48))\n",
        "    x = down(x, 'pool2'); pool2 = x\n",
        "\n",
        "    x = LR(conv(x, 'enc_conv3', 48))\n",
        "    x = down(x, 'pool3'); pool3 = x\n",
        "\n",
        "    x = LR(conv(x, 'enc_conv4', 48))\n",
        "    x = down(x, 'pool4'); pool4 = x\n",
        "\n",
        "    x = LR(conv(x, 'enc_conv5', 48))\n",
        "    x = down(x, 'pool5')\n",
        "\n",
        "    x = LR(conv(x, 'enc_conv6', 48))\n",
        "\n",
        "    # Decoder part.\n",
        "\n",
        "    x = up(x, 'upsample5')\n",
        "    x = concat('concat5', [x, pool4])\n",
        "    x = LR(conv(x, 'dec_conv5a', 96))\n",
        "    x = LR(conv(x, 'dec_conv5b', 96))\n",
        "\n",
        "    x = up(x, 'upsample4')\n",
        "    x = concat('concat4', [x, pool3])\n",
        "    x = LR(conv(x, 'dec_conv4a', 96))\n",
        "    x = LR(conv(x, 'dec_conv4b', 96))\n",
        "\n",
        "    x = up(x, 'upsample3')\n",
        "    x = concat('concat3', [x, pool2])\n",
        "    x = LR(conv(x, 'dec_conv3a', 96))\n",
        "    x = LR(conv(x, 'dec_conv3b', 96))\n",
        "\n",
        "    x = up(x, 'upsample2')\n",
        "    x = concat('concat2', [x, pool1])\n",
        "    x = LR(conv(x, 'dec_conv2a', 96))\n",
        "    x = LR(conv(x, 'dec_conv2b', 96))\n",
        "\n",
        "    x = up(x, 'upsample1')\n",
        "    x = concat('concat1', [x, pool0])\n",
        "\n",
        "    # Output stages.\n",
        "\n",
        "    if blindspot:\n",
        "        # Blind-spot output stages.\n",
        "        x = LR(conv(x, 'dec_conv1a', 96))\n",
        "        x = LR(conv(x, 'dec_conv1b', 96))\n",
        "        x = tf.pad(x[:, :, :-1, :], [[0, 0], [0, 0], [1, 0], [0, 0]])   # Shift and pad.\n",
        "        x = tf.split(x, 4, axis=0)                                      # Split into rotations.\n",
        "        x = [rotate(y, a) for y, a in zip(x, [0, 270, 180, 90])]        # Counterrotate.\n",
        "        x = tf.concat(x, axis=1)                                        # Combine on channel axis.\n",
        "        x = LR(conv(x, 'nin_a', 96*4, size=1))\n",
        "        x = LR(conv(x, 'nin_b', 96, size=1))\n",
        "        x = conv(x, 'nin_c', num_output_components, size=1, gain=1.0, zero_weights=zero_last)\n",
        "    else:\n",
        "        # Baseline network with postprocessing layers -- keep feature maps and distill with 1x1 convolutions.\n",
        "        x = LR(conv(x, 'dec_conv1a', 96))\n",
        "        x = LR(conv(x, 'dec_conv1b', 96))\n",
        "        x = LR(conv(x, 'nin_a', 96, size=1))\n",
        "        x = LR(conv(x, 'nin_b', 96, size=1))\n",
        "        x = conv(x, 'nin_c', num_output_components, size=1, gain=1.0, zero_weights=zero_last)\n",
        "\n",
        "    # Return results.\n",
        "\n",
        "    return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def blindspot_pipeline(noisy_in,\n",
        "                       noise_params_in,\n",
        "                       diagonal_covariance  = False,\n",
        "                       input_shape          = None,\n",
        "                       noise_style          = None,\n",
        "                       noise_params         = None,\n",
        "                       **_kwargs):\n",
        "\n",
        "    num_channels = input_shape[1]\n",
        "    assert num_channels in [1, 3]\n",
        "    assert noise_style in ['gauss', 'poisson', 'impulse']\n",
        "    assert noise_params in ['known', 'global', 'per_image']\n",
        "\n",
        "    # Shapes.\n",
        "    noisy_in.set_shape(input_shape)\n",
        "    noise_params_in.set_shape(input_shape[:1] + [1, 1, 1])\n",
        "\n",
        "    # Clean data distribution.\n",
        "    num_output_components = num_channels + (num_channels * (num_channels + 1)) // 2 # Means, triangular A.\n",
        "    if diagonal_covariance:\n",
        "        num_output_components = num_channels * 2 # Means, diagonal of A.\n",
        "    net_out = analysis_network(noisy_in, num_output_components, blindspot=True)\n",
        "    net_out = tf.cast(net_out, tf.float64)\n",
        "    mu_x = net_out[:, 0:num_channels, ...] # Means (NCHW).\n",
        "    A_c = net_out[:, num_channels:num_output_components, ...] # Components ot triangular A.\n",
        "    if num_channels == 1:\n",
        "        sigma_x = A_c ** 2 # N1HW\n",
        "    elif num_channels == 3:\n",
        "        A_c = tf.transpose(A_c, [0, 2, 3, 1]) # NHWC\n",
        "        if diagonal_covariance:\n",
        "            c00 = A_c[..., 0]**2\n",
        "            c11 = A_c[..., 1]**2\n",
        "            c22 = A_c[..., 2]**2\n",
        "            zro = tf.zeros_like(c00)\n",
        "            c0 = tf.stack([c00, zro, zro], axis=-1) # NHW3\n",
        "            c1 = tf.stack([zro, c11, zro], axis=-1) # NHW3\n",
        "            c2 = tf.stack([zro, zro, c22], axis=-1) # NHW3\n",
        "        else:\n",
        "            # Calculate A^T * A\n",
        "            c00 = A_c[..., 0]**2 + A_c[..., 1]**2 + A_c[..., 2]**2 # NHW\n",
        "            c01 = A_c[..., 1]*A_c[..., 3] + A_c[..., 2]*A_c[..., 4]\n",
        "            c02 = A_c[..., 2]*A_c[..., 5]\n",
        "            c11 = A_c[..., 3]**2 + A_c[..., 4]**2\n",
        "            c12 = A_c[..., 4]*A_c[..., 5]\n",
        "            c22 = A_c[..., 5]**2\n",
        "            c0 = tf.stack([c00, c01, c02], axis=-1) # NHW3\n",
        "            c1 = tf.stack([c01, c11, c12], axis=-1) # NHW3\n",
        "            c2 = tf.stack([c02, c12, c22], axis=-1) # NHW3\n",
        "        sigma_x = tf.stack([c0, c1, c2], axis=-1) # NHW33\n",
        "\n",
        "    # Data on which noise parameter estimation is based.\n",
        "    if noise_params == 'global':\n",
        "        # Global constant over the entire dataset.\n",
        "        noise_est_out = tf.get_variable('noise_data', shape=[1, 1, 1, 1], initializer=tf.initializers.constant(0.0)) # 1111\n",
        "        noise_est_out = tf.cast(noise_est_out, tf.float64)\n",
        "    elif noise_params == 'per_image':\n",
        "        # Separate analysis network.\n",
        "        with tf.variable_scope('param_estimation_net'):\n",
        "            noise_est_out = analysis_network(noisy_in, 1, blindspot=False, zero_last=True) # N1HW\n",
        "        noise_est_out = tf.reduce_mean(noise_est_out, axis=[2, 3], keepdims=True) # N111\n",
        "        noise_est_out = tf.cast(noise_est_out, tf.float64)\n",
        "\n",
        "    # Cast remaining data into float64.\n",
        "    noisy_in = tf.cast(noisy_in, tf.float64)\n",
        "    noise_params_in = tf.cast(noise_params_in, tf.float64)\n",
        "\n",
        "    # Remap noise estimate to ensure it is always positive and starts near zero.\n",
        "    if noise_params != 'known':\n",
        "        noise_est_out = tf.nn.softplus(noise_est_out - 4.0) + 1e-3\n",
        "\n",
        "    # Distill noise parameters from learned/known data.\n",
        "    if noise_style == 'gauss':\n",
        "        if noise_params == 'known':\n",
        "            noise_std = tf.maximum(noise_params_in, 1e-3) # N111\n",
        "        else:\n",
        "            noise_std = noise_est_out\n",
        "    elif noise_style == 'poisson': # Simple signal-dependent Poisson approximation [Hasinoff 2012].\n",
        "        if noise_params == 'known':\n",
        "            noise_std = (tf.maximum(mu_x, tf.constant(1e-3, tf.float64)) / noise_params_in) ** 0.5 # NCHW\n",
        "        else:\n",
        "            noise_std = (tf.maximum(mu_x, tf.constant(1e-3, tf.float64)) * noise_est_out) ** 0.5 # NCHW\n",
        "    elif noise_style == 'impulse':\n",
        "        if noise_params == 'known':\n",
        "            noise_std = noise_params_in # N111, actually the alpha.\n",
        "        else:\n",
        "            noise_std = noise_est_out\n",
        "\n",
        "    # Casts and vars.\n",
        "    noise_std = tf.cast(noise_std, tf.float64)\n",
        "    I = tf.eye(num_channels, batch_shape=[1, 1, 1], dtype=tf.float64)\n",
        "    Ieps = I * tf.constant(1e-6, dtype=tf.float64)\n",
        "    zero64 = tf.constant(0.0, dtype=tf.float64)\n",
        "\n",
        "    # Helpers.\n",
        "    def batch_mvmul(m, v): # Batched (M * v).\n",
        "        return tf.reduce_sum(m * v[..., tf.newaxis, :], axis=-1)\n",
        "    def batch_vtmv(v, m): # Batched (v^T * M * v).\n",
        "        return tf.reduce_sum(v[..., :, tf.newaxis] * v[..., tf.newaxis, :] * m, axis=[-2, -1])\n",
        "    def batch_vvt(v): # Batched (v * v^T).\n",
        "        return v[..., :, tf.newaxis] * v[..., tf.newaxis, :]\n",
        "\n",
        "    # Negative log-likelihood loss and posterior mean estimation.\n",
        "    if noise_style in ['gauss', 'poisson']:\n",
        "        if num_channels == 1:\n",
        "            sigma_n = noise_std**2 # N111 / N1HW\n",
        "            sigma_y = sigma_x + sigma_n # N1HW. Total variance.\n",
        "            loss_out = ((noisy_in - mu_x) ** 2) / sigma_y + tf.log(sigma_y) # N1HW\n",
        "            pme_out = (noisy_in * sigma_x + mu_x * sigma_n) / (sigma_x + sigma_n) # N1HW\n",
        "            net_std_out = (sigma_x**0.5)[:, 0, ...] # NHW\n",
        "            noise_std_out = noise_std[:, 0, ...] # N11 / NHW\n",
        "            if noise_params != 'known':\n",
        "                loss_out = loss_out - 0.1 * noise_std # Balance regularization.\n",
        "        else:\n",
        "            # Training loss.\n",
        "            sigma_n = tf.transpose(noise_std**2, [0, 2, 3, 1])[..., tf.newaxis] * I # NHWC1 * NHWCC = NHWCC\n",
        "            sigma_y = sigma_x + sigma_n # NHWCC, total covariance matrix. Cannot be singular because sigma_n is at least a small diagonal.\n",
        "            sigma_y_inv = tf.linalg.inv(sigma_y) # NHWCC\n",
        "            mu_x2 = tf.transpose(mu_x, [0, 2, 3, 1]) # NHWC\n",
        "            noisy_in2 = tf.transpose(noisy_in, [0, 2, 3, 1]) # NHWC\n",
        "            diff = (noisy_in2 - mu_x2) # NHWC\n",
        "            diff = -0.5 * batch_vtmv(diff, sigma_y_inv) # NHW\n",
        "            dets = tf.linalg.det(sigma_y) # NHW\n",
        "            dets = tf.maximum(zero64, dets) # NHW. Avoid division by zero and negative square roots.\n",
        "            loss_out = 0.5 * tf.log(dets) - diff # NHW\n",
        "            if noise_params != 'known':\n",
        "                loss_out = loss_out - 0.1 * tf.reduce_mean(noise_std, axis=1) # Balance regularization.\n",
        "\n",
        "            # Posterior mean estimate.\n",
        "            sigma_x_inv = tf.linalg.inv(sigma_x + Ieps) # NHWCC\n",
        "            sigma_n_inv = tf.linalg.inv(sigma_n + Ieps) # NHWCC\n",
        "            pme_c1 = tf.linalg.inv(sigma_x_inv + sigma_n_inv + Ieps) # NHWCC\n",
        "            pme_c2 = batch_mvmul(sigma_x_inv, mu_x2) # NHWCC * NHWC -> NHWC\n",
        "            pme_c2 = pme_c2 + batch_mvmul(sigma_n_inv, noisy_in2) # NHWC\n",
        "            pme_out = batch_mvmul(pme_c1, pme_c2) # NHWC\n",
        "            pme_out = tf.transpose(pme_out, [0, 3, 1, 2]) # NCHW\n",
        "\n",
        "            # Summary statistics.\n",
        "            net_std_out = tf.maximum(zero64, tf.linalg.det(sigma_x))**(1.0/6.0) # NHW\n",
        "            noise_std_out = tf.maximum(zero64, tf.linalg.det(sigma_n))**(1.0/6.0) # N11 / NHW\n",
        "    elif noise_style == 'impulse':\n",
        "        alpha = noise_std # N111.\n",
        "        if num_channels == 1:\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            # Preliminaries.\n",
        "            sigma_x = sigma_x + Ieps # NHWCC. Inflate by epsilon.\n",
        "            sigma_x_inv = tf.linalg.inv(sigma_x) # NHWCC\n",
        "            mu_x2 = tf.transpose(mu_x, [0, 2, 3, 1]) # NHWC\n",
        "            noisy_in2 = tf.transpose(noisy_in, [0, 2, 3, 1]) # NHWC\n",
        "            diff = (noisy_in2 - mu_x2) # NHWC\n",
        "            diff = batch_vtmv(diff, sigma_x_inv) # NHW\n",
        "            dets = tf.linalg.det(sigma_x) # NHW\n",
        "            dets = tf.maximum(tf.constant(1e-9, dtype=tf.float64), dets) # NHW. Avoid division by zero and negative square roots.\n",
        "            g = tf.exp(-0.5 * diff) / ((2.0 * np.pi)**num_channels * dets)**0.5 # NHW\n",
        "            g = g[..., tf.newaxis] # NHW1\n",
        "\n",
        "            # Posterior mean estimate.\n",
        "            h = (1.0 - alpha) * g # NHW1\n",
        "            pme_out = (alpha * mu_x2 + h * noisy_in2) / (alpha + h)\n",
        "            pme_out = tf.transpose(pme_out, [0, 3, 1, 2]) # NCHW\n",
        "\n",
        "            # Training loss with the modified stats.\n",
        "            mu_y2 = alpha * .5 + (1.0 - alpha) * mu_x2 # NHWC\n",
        "            alpha = alpha[..., tf.newaxis] # n1111\n",
        "            sigma_y = alpha * (1.0/4.0 + I/12.0) + (1.0 - alpha) * (sigma_x + batch_vvt(mu_x2)) - batch_vvt(mu_y2) # NHWCC\n",
        "            sigma_y_inv = tf.linalg.inv(sigma_y) # NHWCC\n",
        "            diff = (noisy_in2 - mu_y2) # NHWC\n",
        "            diff = batch_vtmv(diff, sigma_y_inv) # NHW\n",
        "            dets = tf.linalg.det(sigma_y) # NHW\n",
        "            dets = tf.maximum(tf.constant(1e-9, dtype=tf.float64), dets) # NHW\n",
        "            loss_out = diff + tf.log(dets) # NHW\n",
        "\n",
        "            # Summary statistics.\n",
        "            net_std_out = tf.maximum(zero64, tf.linalg.det(sigma_x))**(1.0/6.0) # NHW. Cube root of volumetric scaling factor.\n",
        "            noise_std_out = alpha[..., 0, 0] / 255.0 * 100.0 # N11 / NHW. Shows as percentage in output.\n",
        "\n",
        "    return mu_x, pme_out, loss_out, net_std_out, noise_std_out\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def simple_pipeline(clean_in,\n",
        "                    noisy_in,\n",
        "                    L_exponent_in,\n",
        "                    noise_style = None,\n",
        "                    input_shape = None,\n",
        "                    blindspot = False,\n",
        "                    noisy_targets = False,\n",
        "                    **_kwargs):\n",
        "\n",
        "    clean_in.set_shape(input_shape)\n",
        "    noisy_in.set_shape(input_shape)\n",
        "    L_exponent_in.set_shape([])\n",
        "\n",
        "    x = analysis_network(noisy_in, input_shape[1], blindspot=blindspot)\n",
        "\n",
        "    if noise_style == 'impulse' and noisy_targets: # Cannot use L2 loss because mean changes\n",
        "        loss_out = (tf.abs(x - clean_in) + 1e-8) ** L_exponent_in\n",
        "    else:\n",
        "        loss_out = (x - clean_in) ** 2.0\n",
        "\n",
        "    net_std_out, noise_std_out = [tf.zeros_like(noisy_in) for x in range(2)]\n",
        "    return x, x, loss_out, net_std_out, noise_std_out\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def get_scrambled_indices(num, bs):\n",
        "    assert num > 0\n",
        "    i, x = 0, []\n",
        "    while True:\n",
        "        res = x[i : i + bs]\n",
        "        i += bs\n",
        "        while len(res) < bs:\n",
        "            x = list(np.arange(num))\n",
        "            np.random.shuffle(x)\n",
        "            i = bs - len(res)\n",
        "            res += x[:i]\n",
        "        yield res\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def random_crop_numpy(img, crop_size):\n",
        "    y = np.random.randint(img.shape[1] - crop_size + 1)\n",
        "    x = np.random.randint(img.shape[2] - crop_size + 1)\n",
        "    return img[:, y : y+crop_size, x : x+crop_size]\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Noise implementations.\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "operation_seed_counter = 0\n",
        "def noisify(x, style):\n",
        "    def get_seed():\n",
        "        global operation_seed_counter\n",
        "        operation_seed_counter += 1\n",
        "        return operation_seed_counter\n",
        "\n",
        "    if style.startswith('gauss'): # Gaussian noise with constant/variable std.dev.\n",
        "        params = [float(p) / 255.0 for p in style.replace('gauss', '', 1).split('_')]\n",
        "        if len(params) == 1:\n",
        "            std = params[0]\n",
        "        elif len(params) == 2:\n",
        "            min_std, max_std = params\n",
        "            std = tf.random_uniform(shape=[tf.shape(x)[0], 1, 1, 1], minval=min_std, maxval=max_std, seed=get_seed())\n",
        "        return x + tf.random_normal(shape=tf.shape(x), seed=get_seed()) * std, std\n",
        "    elif style.startswith('poisson'): # Poisson noise with constant/variable lambda.\n",
        "        params = [float(p) for p in style.replace('poisson', '', 1).split('_')]\n",
        "        if len(params) == 1:\n",
        "            lam = params[0]\n",
        "        elif len(params) == 2:\n",
        "            min_lam, max_lam = params\n",
        "            lam = tf.random_uniform(shape=[tf.shape(x)[0], 1, 1, 1], minval=min_lam, maxval=max_lam, seed=get_seed())\n",
        "        x = x * lam\n",
        "        with tf.device(\"/cpu:0\"):\n",
        "            x = tf.random_poisson(x, [1], seed=get_seed())\n",
        "        return x[0] / lam, lam\n",
        "    elif style.startswith('impulse'): # Random replacement with constant/variable alpha.\n",
        "        params = [float(p) * 0.01 for p in style.replace('impulse', '', 1).split('_')]\n",
        "        msh = tf.shape(x[:, :1, ...])\n",
        "        if len(params) == 1:\n",
        "            alpha = params[0]\n",
        "            keep_mask = tf.where(tf.random_uniform(shape=msh, seed=get_seed()) >= alpha, tf.ones(shape=msh), tf.zeros(shape=msh))\n",
        "        elif len(params) == 2:\n",
        "            min_alpha, max_alpha = params\n",
        "            alpha = tf.random_uniform(shape=[tf.shape(x)[0], 1, 1, 1], minval=min_alpha, maxval=max_alpha, seed=get_seed())\n",
        "            keep_mask = tf.where(tf.random_uniform(shape=msh, seed=get_seed()) >= tf.ones(shape=msh) * alpha, tf.ones(shape=msh), tf.zeros(shape=msh))\n",
        "        noise = tf.random_uniform(shape=tf.shape(x), seed=get_seed())\n",
        "        return x * keep_mask + noise * (1.0 - keep_mask), alpha\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Training loop.\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "def train(submit_config,\n",
        "          num_iter              = 1000000,\n",
        "          train_resolution      = 256,\n",
        "          minibatch_size        = 4,\n",
        "          learning_rate         = 3e-4,\n",
        "          rampup_fraction       = 0.1,\n",
        "          rampdown_fraction     = 0.3,\n",
        "          snapshot_every        = 0,        # Export network snapshot every n images (must be divisible by minibatch).\n",
        "          pipeline              = None,\n",
        "          diagonal_covariance   = False,    # Force non-diagonal covariances to zero (per-channel univariate).\n",
        "          noise_style           = None,\n",
        "          noise_params          = None,     # 'known', 'global', 'per_image'\n",
        "          train_dataset         = None,\n",
        "          validation_dataset    = None,\n",
        "          validation_repeats    = 1,\n",
        "          prune_dataset         = None,\n",
        "          num_channels          = None,\n",
        "          print_interval        = 1000,\n",
        "          eval_interval         = 10000,\n",
        "          eval_network          = None,\n",
        "          config_name           = None,\n",
        "          dataset_dir           = None):\n",
        "\n",
        "    # Are we in evaluation mode?\n",
        "    eval_mode = eval_network is not None\n",
        "\n",
        "    # Initialize Tensorflow.\n",
        "    if eval_mode:\n",
        "        init_tf(0) # Use fixed seeds if evaluating a network.\n",
        "        np.random.seed(0)\n",
        "    else:\n",
        "        init_tf() # Use a random random seed.\n",
        "\n",
        "    # Get going.\n",
        "    ctx = dnnlib.RunContext(submit_config)\n",
        "    run_dir = submit_config.run_dir\n",
        "    img_dir = os.path.join(run_dir, 'img')\n",
        "    os.makedirs(img_dir, exist_ok=True)\n",
        "\n",
        "    # Load the data.\n",
        "    train_images, validation_images, validation_image_size = load_datasets(num_channels, dataset_dir, None if eval_mode else train_dataset, validation_dataset, prune_dataset)\n",
        "\n",
        "    # Repeat validation set if asked to.\n",
        "    original_validation_image_count = len(validation_images) # Avoid exporting the duplicate images.\n",
        "    if validation_repeats > 1:\n",
        "        print(\"Repeating the validation set %d times.\" % validation_repeats)\n",
        "        validation_images = validation_images * validation_repeats\n",
        "        validation_image_size = validation_image_size * validation_repeats\n",
        "\n",
        "    # Construct the network.\n",
        "    input_shape = [None, num_channels, None, None]\n",
        "    with tf.device(\"/gpu:0\"):\n",
        "        if eval_mode:\n",
        "            print(\"Evaluating network '%s'.\" % eval_network)\n",
        "            with open(eval_network, 'rb') as f:\n",
        "                net = pickle.load(f)\n",
        "        else:\n",
        "            if noise_style.startswith('gauss'):   net_noise_style = 'gauss'\n",
        "            if noise_style.startswith('poisson'): net_noise_style = 'poisson'\n",
        "            if noise_style.startswith('impulse'): net_noise_style = 'impulse'\n",
        "\n",
        "            if pipeline == 'blindspot':\n",
        "                net = dnnlib.tflib.Network('net', 'selfsupervised_denoising.blindspot_pipeline', input_shape=input_shape, noise_style=net_noise_style, noise_params=noise_params, diagonal_covariance=diagonal_covariance)\n",
        "            elif pipeline == 'blindspot_mean':\n",
        "                net = dnnlib.tflib.Network('net', 'selfsupervised_denoising.simple_pipeline', input_shape=input_shape, noise_style=net_noise_style, blindspot=True, noisy_targets=True)\n",
        "            elif pipeline == 'n2c':\n",
        "                net = dnnlib.tflib.Network('net', 'selfsupervised_denoising.simple_pipeline', input_shape=input_shape, noise_style=net_noise_style, blindspot=False, noisy_targets=False)\n",
        "            elif pipeline == 'n2n':\n",
        "                net = dnnlib.tflib.Network('net', 'selfsupervised_denoising.simple_pipeline', input_shape=input_shape, noise_style=net_noise_style, blindspot=False, noisy_targets=True)\n",
        "\n",
        "    # Data splits.\n",
        "    with tf.name_scope('Inputs'), tf.device(\"/cpu:0\"):\n",
        "        learning_rate_in = tf.placeholder(tf.float32, name='learning_rate_in', shape=[])\n",
        "        L_exponent_in = tf.placeholder(tf.float32, name='L_exponent', shape=[])\n",
        "        clean_in = tf.placeholder(tf.float32, shape=input_shape)\n",
        "        clean_in_split = tf.split(clean_in, submit_config.num_gpus)\n",
        "\n",
        "    # Optimizer.\n",
        "    opt = dnnlib.tflib.Optimizer(tf_optimizer='tf.train.AdamOptimizer', learning_rate=learning_rate_in, beta1=0.9, beta2=0.99)\n",
        "\n",
        "    # Per-gpu stuff.\n",
        "    train_loss = 0.\n",
        "    train_psnr = 0.\n",
        "    train_psnr_pme = 0.\n",
        "    gpu_outputs = []\n",
        "    for gpu in range(submit_config.num_gpus):\n",
        "        with tf.device(\"/gpu:%d\" % gpu):\n",
        "            net_gpu = net if gpu == 0 else net.clone()\n",
        "            clean_in_gpu = clean_in_split[gpu]\n",
        "            noisy_in_gpu, noise_coeff = noisify(clean_in_gpu, noise_style)\n",
        "\n",
        "            if pipeline == 'blindspot_mean':\n",
        "                reference_in_gpu = noisy_in_gpu\n",
        "            elif pipeline == 'n2n':\n",
        "                reference_in_gpu, _ = noisify(clean_in_gpu, noise_style) # Another noise instantiation.\n",
        "            else:\n",
        "                reference_in_gpu = clean_in_gpu\n",
        "\n",
        "            noise_coeff = tf.zeros([tf.shape(noisy_in_gpu)[0], 1, 1, 1]) + noise_coeff # Broadcast to [n, 1, 1, 1] shape.\n",
        "\n",
        "            # Support for networks that were exported from an older version of code and loaded for evaluation purposes.\n",
        "            if net.num_inputs == 5:\n",
        "                mu_x, pme_out, loss_out, net_std_out, noise_std_out, _ = net_gpu.get_output_for(reference_in_gpu, noisy_in_gpu, noise_coeff, tf.constant(1e-6, dtype=tf.float32), tf.constant(1e-1, dtype=tf.float32))\n",
        "            else:\n",
        "                if pipeline == 'blindspot':\n",
        "                   if net.num_inputs == 3:\n",
        "                        mu_x, pme_out, loss_out, net_std_out, noise_std_out, _ = net_gpu.get_output_for(noisy_in_gpu, noise_coeff, L_exponent_in) # Previous version.\n",
        "                   else:\n",
        "                        mu_x, pme_out, loss_out, net_std_out, noise_std_out = net_gpu.get_output_for(noisy_in_gpu, noise_coeff)\n",
        "                else:\n",
        "                    if net.num_inputs == 4:\n",
        "                        mu_x, pme_out, loss_out, net_std_out, noise_std_out, _ = net_gpu.get_output_for(reference_in_gpu, noisy_in_gpu, noise_coeff, L_exponent_in) # Previous version.\n",
        "                    else:\n",
        "                        mu_x, pme_out, loss_out, net_std_out, noise_std_out = net_gpu.get_output_for(reference_in_gpu, noisy_in_gpu, L_exponent_in)\n",
        "\n",
        "            gpu_outputs.append([mu_x, pme_out, loss_out, net_std_out, noise_std_out, noisy_in_gpu])\n",
        "\n",
        "            # Loss.\n",
        "            loss = tf.reduce_mean(loss_out)\n",
        "\n",
        "             # PSNR during training.\n",
        "            psnr = tf.reduce_mean(calculate_psnr(mu_x, clean_in_gpu, axis=[1, 2, 3]))\n",
        "            psnr_pme = tf.reduce_mean(calculate_psnr(pme_out, clean_in_gpu, axis=[1, 2, 3]))\n",
        "            with tf.control_dependencies([autosummary(\"train_loss\", loss), autosummary(\"train_psnr\", psnr), autosummary(\"train_psnr_pme\", psnr_pme)]):\n",
        "                opt.register_gradients(loss, net_gpu.trainables)\n",
        "\n",
        "        # Accumulation not on the GPU.\n",
        "        train_loss += loss / submit_config.num_gpus\n",
        "        train_psnr += psnr / submit_config.num_gpus\n",
        "        train_psnr_pme += psnr_pme / submit_config.num_gpus\n",
        "\n",
        "    # Total outputs.\n",
        "    mu_x_out, pme_out, loss_out, net_std_out, noise_std_out, noisy_out = [tf.concat(x, axis=0) for x in zip(*gpu_outputs)]\n",
        "\n",
        "    # Train step op.\n",
        "    train_step = opt.apply_updates()\n",
        "\n",
        "    # Create a log file for Tensorboard.\n",
        "    if not eval_mode:\n",
        "        summary_log = tf.summary.FileWriter(run_dir)\n",
        "        summary_log.add_graph(tf.get_default_graph())\n",
        "\n",
        "    # Training image index generator.\n",
        "    index_generator = get_scrambled_indices(len(train_images), minibatch_size)\n",
        "\n",
        "    # Init stats.\n",
        "    print_last, eval_last = 0, 0\n",
        "    loss_acc, loss_n = 0., 0.\n",
        "    psnr_acc, psnr_pme_acc = 0., 0.\n",
        "    std_net_acc, std_noise_acc = 0., 0.\n",
        "    valid_psnr_mu, valid_psnr_pme = 0., 0.\n",
        "    t_start = time.time()\n",
        "\n",
        "    # Train.\n",
        "    if eval_mode:\n",
        "        print('Evaluating network with %d images.' % len(validation_images))\n",
        "    else:\n",
        "        print('Training for %d images.' % num_iter)\n",
        "\n",
        "    for n in range(0, num_iter + minibatch_size, minibatch_size):\n",
        "        if ctx.should_stop():\n",
        "            break\n",
        "\n",
        "        # Save snapshot.\n",
        "        if (n > 0) and (snapshot_every > 0) and (n % snapshot_every == 0):\n",
        "            save_snapshot(submit_config, net, '%08d' % n)\n",
        "\n",
        "        # Set up training step.\n",
        "        lr = compute_ramped_lrate(n, num_iter, rampup_fraction, rampdown_fraction, learning_rate)\n",
        "        L_exponent = 0.5 if eval_mode else max(0.5, 2.0 - 2.0 * n / num_iter)\n",
        "\n",
        "        # Training step unless in evaluation mode.\n",
        "        if not eval_mode:\n",
        "            # Get clean images from training set.\n",
        "            clean = np.zeros([minibatch_size, num_channels, train_resolution, train_resolution], dtype=np.uint8)\n",
        "            for i, j in enumerate(next(index_generator)):\n",
        "                clean[i] = random_crop_numpy(train_images[j], train_resolution)\n",
        "            clean = adjust_dynamic_range(clean, [0, 255], [0.0, 1.0])\n",
        "\n",
        "            # Run training step.\n",
        "            feed_dict = {clean_in: clean, learning_rate_in: lr, L_exponent_in: L_exponent}\n",
        "            loss_val, psnr_val, psnr_pme_val, net_std_val, noise_std_val, _ = tfutil.run([train_loss, train_psnr, train_psnr_pme, net_std_out, noise_std_out, train_step], feed_dict)\n",
        "\n",
        "            # Accumulate stats.\n",
        "            loss_acc += loss_val\n",
        "            psnr_acc += psnr_val\n",
        "            psnr_pme_acc += psnr_pme_val\n",
        "            std_net_acc += np.mean(net_std_val)\n",
        "            std_noise_acc += np.mean(noise_std_val)\n",
        "            loss_n += 1.0\n",
        "\n",
        "        # Print.\n",
        "        if n == 0 or n >= print_last + print_interval:\n",
        "            loss_n = max(loss_n, 1.0)\n",
        "            loss_acc /= loss_n\n",
        "            psnr_acc /= loss_n\n",
        "            psnr_pme_acc /= loss_n\n",
        "            std_net_acc = std_net_acc / loss_n * 255.0\n",
        "            std_noise_acc = std_noise_acc / loss_n * 255.0\n",
        "            t_iter = time.time() - t_start\n",
        "            print(\"%8d: time=%6.2f, loss=%8.4f, train_psnr=%8.4f, train_psnr_pme=%8.4f, std_net=%8.4f, std_noise=%8.4f\" % (n, t_iter, loss_acc, psnr_acc, psnr_pme_acc, autosummary('std_net', std_net_acc), autosummary('std_noise', std_noise_acc)), end='')\n",
        "            ctx.update(loss='%.2f %.2f' % (psnr_pme_acc, valid_psnr_pme), cur_epoch=n, max_epoch=num_iter)\n",
        "            print_last += print_interval if (n > 0) else 0\n",
        "            loss_acc, loss_n = 0., 0.\n",
        "            psnr_acc, psnr_pme_acc = 0., 0.\n",
        "            std_net_acc, std_noise_acc = 0., 0.\n",
        "            t_start = time.time()\n",
        "\n",
        "            # Measure and export validation images.\n",
        "            if n == 0 or n >= eval_last + eval_interval or n == num_iter:\n",
        "                valid_psnr_mu = 0.\n",
        "                valid_psnr_pme = 0.\n",
        "                bs = submit_config.num_gpus # Validation batch size.\n",
        "                for idx0 in range(0, len(validation_images), bs):\n",
        "                    num = min(bs, len(validation_images) - idx0)\n",
        "                    idx = list(range(idx0, idx0 + bs))\n",
        "                    idx = [min(x, len(validation_images) - 1) for x in idx]\n",
        "                    val_input = []\n",
        "                    val_sz = []\n",
        "                    for i in idx:\n",
        "                        img = validation_images[i][np.newaxis, ...]\n",
        "                        img = adjust_dynamic_range(img, [0, 255], [0.0, 1.0])\n",
        "                        sz = img.shape[2:]\n",
        "                        img = np.pad(img, [[0, 0], [0, 0], [0, validation_image_size[0] - sz[0]], [0, validation_image_size[1] - sz[1]]], 'reflect')\n",
        "                        val_input.append(img)\n",
        "                        val_sz.append(sz)\n",
        "                    val_input = np.concatenate(val_input, axis=0) # Batch of validation images.\n",
        "\n",
        "                    # Run the actual step.\n",
        "                    feed_dict = {clean_in: val_input}\n",
        "                    mu_x, net_std, pme, noisy = tfutil.run([mu_x_out, net_std_out, pme_out, noisy_out], feed_dict)\n",
        "\n",
        "                    # Process the result images.\n",
        "                    for i, j in enumerate(idx[:num]):\n",
        "                        crop_val_input, crop_mu_x, crop_pme, crop_noisy = [x[i, :, :val_sz[i][0], :val_sz[i][1]] for x in [val_input, mu_x, pme, noisy]]\n",
        "                        crop_net_std = net_std[i, :val_sz[i][0], :val_sz[i][1]] # HW grayscale\n",
        "                        crop_net_std /= 10.0 / 255.0 # white = 10 ULPs in U8.\n",
        "                        valid_psnr_mu += calculate_psnr(crop_mu_x, crop_val_input) / len(validation_images)\n",
        "                        valid_psnr_pme += calculate_psnr(crop_pme, crop_val_input) / len(validation_images)\n",
        "\n",
        "                        if (eval_mode and (j < original_validation_image_count)) or ((not eval_mode) and (j == len(validation_images) - 1)): # Export last image, or all if evaluating.\n",
        "                            k, ext = (j, 'png') if eval_mode else (n, 'jpg')\n",
        "                            def save_img(name, img): save_image(img, os.path.join(img_dir, 'img-%07d-%s.%s' % (k, name, ext)), [0.0, 1.0])\n",
        "                            save_img('a_nsy',  crop_noisy)      # Noisy input\n",
        "                            save_img('b_out',  crop_mu_x)       # Predicted mean\n",
        "                            save_img('b_out2', crop_pme)        # Posterior mean estimate (actual output)\n",
        "                            save_img('b_std',  crop_net_std)    # Predicted std. dev\n",
        "                            save_img('c_cln',  crop_val_input)  # Clean reference image\n",
        "\n",
        "                    # Validation pass completed.\n",
        "\n",
        "                print(\", valid_psnr_mu=%8.4f, valid_psnr_pme=%8.4f\" % (valid_psnr_mu, valid_psnr_pme), end='')\n",
        "                eval_last += eval_interval if (n > 0) else 0\n",
        "\n",
        "            # Exit if evaluation mode.\n",
        "            if eval_mode:\n",
        "                print(\"\\nEvaluation done, exiting.\")\n",
        "                print(\"RESULT %8.4f\" % valid_psnr_pme)\n",
        "                ctx.close()\n",
        "                return\n",
        "\n",
        "            # Finish printing.\n",
        "            autosummary('valid_psnr_mu', valid_psnr_mu)\n",
        "            autosummary('valid_psnr_pme', valid_psnr_pme)\n",
        "            dnnlib.tflib.autosummary.save_summaries(summary_log, n)\n",
        "            print(\"\")\n",
        "\n",
        "    # Save the result.\n",
        "    save_snapshot(submit_config, net, 'final-'+config_name)\n",
        "\n",
        "    # Done.\n",
        "    summary_log.close()\n",
        "    ctx.close()\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "config_lst = [\n",
        "    dict(eval_id = '00011', noise_style='gauss25', num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00012', noise_style='gauss25', num_iter=2000000, pipeline='n2n'),\n",
        "    dict(eval_id = '00013', noise_style='gauss25', num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00014', noise_style='gauss25', num_iter=2000000, pipeline='blindspot', noise_params='global'),\n",
        "    dict(eval_id = '00015', noise_style='gauss25', num_iter=2000000, pipeline='blindspot', noise_params='known',  diagonal_covariance=True),\n",
        "    dict(eval_id = '00016', noise_style='gauss25', num_iter=2000000, pipeline='blindspot', noise_params='global', diagonal_covariance=True),\n",
        "    dict(eval_id = '00017', noise_style='gauss25', num_iter=2000000, pipeline='blindspot_mean'),\n",
        "    dict(eval_id = '00018', noise_style='gauss5_50', num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00019', noise_style='gauss5_50', num_iter=2000000, pipeline='n2n'),\n",
        "    dict(eval_id = '00020', noise_style='gauss5_50', num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00021', noise_style='gauss5_50', num_iter=2000000, pipeline='blindspot', noise_params='per_image'),\n",
        "    dict(eval_id = '00022', noise_style='gauss5_50', num_iter=2000000, pipeline='blindspot', noise_params='known',     diagonal_covariance=True),\n",
        "    dict(eval_id = '00023', noise_style='gauss5_50', num_iter=2000000, pipeline='blindspot', noise_params='per_image', diagonal_covariance=True),\n",
        "    dict(eval_id = '00024', noise_style='gauss5_50', num_iter=2000000, pipeline='blindspot_mean'),\n",
        "    dict(eval_id = '00030', noise_style='poisson30', num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00031', noise_style='poisson30', num_iter=2000000, pipeline='n2n'),\n",
        "    dict(eval_id = '00032', noise_style='poisson30', num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00033', noise_style='poisson30', num_iter=2000000, pipeline='blindspot', noise_params='global'),\n",
        "    dict(eval_id = '00034', noise_style='poisson30', num_iter=2000000, pipeline='blindspot_mean'),\n",
        "    dict(eval_id = '00035', noise_style='poisson5_50', num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00036', noise_style='poisson5_50', num_iter=2000000, pipeline='n2n'),\n",
        "    dict(eval_id = '00037', noise_style='poisson5_50', num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00038', noise_style='poisson5_50', num_iter=2000000, pipeline='blindspot', noise_params='per_image'),\n",
        "    dict(eval_id = '00039', noise_style='poisson5_50', num_iter=2000000, pipeline='blindspot_mean'),\n",
        "    dict(eval_id = '00050', noise_style='impulse50', pipeline='n2c', num_iter=16000000),\n",
        "    dict(eval_id = '00051', noise_style='impulse50', pipeline='n2n', num_iter=16000000),\n",
        "    dict(eval_id = '00052', noise_style='impulse50', pipeline='blindspot', noise_params='known',  num_iter=4000000),\n",
        "    dict(eval_id = '00053', noise_style='impulse50', pipeline='blindspot', noise_params='global', num_iter=4000000),\n",
        "    dict(eval_id = '00054', noise_style='impulse50', pipeline='blindspot_mean', num_iter=8000000),\n",
        "    dict(eval_id = '00055', noise_style='impulse0_100', pipeline='n2c', num_iter=16000000),\n",
        "    dict(eval_id = '00056', noise_style='impulse0_100', pipeline='n2n', num_iter=16000000),\n",
        "    dict(eval_id = '00057', noise_style='impulse0_100', pipeline='blindspot', noise_params='known',     num_iter=4000000),\n",
        "    dict(eval_id = '00058', noise_style='impulse0_100', pipeline='blindspot', noise_params='per_image', num_iter=4000000),\n",
        "    dict(eval_id = '00059', noise_style='impulse0_100', pipeline='blindspot_mean',                      num_iter=8000000),\n",
        "    dict(eval_id = '00180', noise_style='gauss25', num_channels=1, num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00181', noise_style='gauss25', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00182', noise_style='gauss25', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='global'),\n",
        "    dict(eval_id = '00183', noise_style='gauss5_50', num_channels=1, num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00184', noise_style='gauss5_50', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00185', noise_style='gauss5_50', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='per_image'),\n",
        "    dict(eval_id = '00188', noise_style='poisson30', num_channels=1, num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00189', noise_style='poisson30', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00190', noise_style='poisson30', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='global'),\n",
        "    dict(eval_id = '00191', noise_style='poisson5_50', num_channels=1, num_iter=2000000, pipeline='n2c'),\n",
        "    dict(eval_id = '00192', noise_style='poisson5_50', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='known'),\n",
        "    dict(eval_id = '00193', noise_style='poisson5_50', num_channels=1, num_iter=2000000, pipeline='blindspot', noise_params='per_image', snapshot_every=100000), # A bit unstable.\n",
        "]\n",
        "\n",
        "\n",
        "def make_config_name(c):\n",
        "    num_channels = c.get('num_channels', 3)\n",
        "    diag = c.get('diagonal_covariance', False)\n",
        "    is_blindspot = c['pipeline'] == 'blindspot'\n",
        "    sigma = '-sigma_'+c['noise_params'] if is_blindspot else ''\n",
        "    return c['noise_style']+'-'+c['pipeline']+('_diag' if diag else '')+sigma+('-mono' if num_channels == 1 else '')\n",
        "\n",
        "# ------------------------------------------------------------------------------------------\n",
        "def cli_examples(configs):\n",
        "    return '''examples:\n",
        "  # Train a network with gauss25-blindspot-sigma_global configuration\n",
        "  python %(prog)s --train=gauss25-blindspot-sigma_global --dataset-dir=$HOME/datasets --validation-set=kodak --train-h5=imagenet_val_raw.h5\n",
        "  # Evaluate a network using the BSD300 dataset:\n",
        "  python %(prog)s --eval=$HOME/pretrained/network-00012-gauss25-n2n.pickle --dataset-dir=$HOME/datasets --validation-set=kodak\n",
        "  List of all configs:\n",
        "  ''' + '\\n  '.join(configs)\n",
        "\n",
        "def main():\n",
        "    sc = dnnlib.SubmitConfig()\n",
        "    sc.run_dir_root = 'results'\n",
        "    sc.run_dir_ignore += ['datasets', 'results']\n",
        "\n",
        "    config_map = {}\n",
        "    selected_config = None\n",
        "    config_names = []\n",
        "    for c in config_lst:\n",
        "        cfg_name = make_config_name(c)\n",
        "        assert cfg_name not in config_map\n",
        "        config_map[cfg_name] = c\n",
        "        config_names.append(cfg_name)\n",
        "\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Train or evaluate.',\n",
        "        epilog=cli_examples(config_names),\n",
        "        formatter_class=argparse.RawDescriptionHelpFormatter\n",
        "    )\n",
        "    parser.add_argument('--dataset-dir', help='Path to validation set data')\n",
        "    parser.add_argument('--train-h5', help='Specify training set .h5 filename')\n",
        "    parser.add_argument('--validation-set', help='Evaluation dataset', default='kodak')\n",
        "    parser.add_argument('--eval', help='Evaluate validation set with the given network pickle')\n",
        "    parser.add_argument('--train', help='Train for the given config')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    eval_sets = {\n",
        "        'kodak':  dict(validation_repeats=10),\n",
        "        'bsd300': dict(validation_repeats=3),\n",
        "        'set14':  dict(validation_repeats=20)\n",
        "    }\n",
        "    if args.validation_set not in eval_sets:\n",
        "        print ('Validation set specified with --validation-set not in one of: ' + ', '.join(eval_sets))\n",
        "        sys.exit(1)\n",
        "\n",
        "    if args.dataset_dir is None:\n",
        "        print ('Must specify validation dataset path with --dataset-dir')\n",
        "        sys.exit(1)\n",
        "    if not os.path.isdir(args.dataset_dir):\n",
        "        print ('Directory specified with --dataset-dir does not seem to exist.')\n",
        "        sys.exit(1)\n",
        "\n",
        "    config_name = None\n",
        "    if args.train:\n",
        "        if args.eval is not None:\n",
        "            print ('Use either --train or --eval')\n",
        "            sys.exit(1)\n",
        "        if args.train_h5 is None:\n",
        "            print ('Must specify training dataset with --train-h5 when training')\n",
        "            sys.exit(1)\n",
        "        config_name = args.train\n",
        "    elif args.eval:\n",
        "        pickle_name = args.eval\n",
        "        pickle_re = re.compile('^network-(?:[0-9]+|final)-(.+)\\\\.pickle')\n",
        "        m = pickle_re.match(os.path.basename(pickle_name))\n",
        "        if m is None:\n",
        "            print ('network pickle name must contain network config string')\n",
        "            sys.exit(1)\n",
        "        config_name = m.group(1)\n",
        "    else:\n",
        "        print ('Must use either --train or --eval')\n",
        "        sys.exit(1)\n",
        "\n",
        "\n",
        "    if config_name not in config_map:\n",
        "        print ('unknown config', config_name)\n",
        "        sys.exit(1)\n",
        "\n",
        "    validation_repeats = eval_sets[args.validation_set]['validation_repeats'] if args.eval else 1\n",
        "\n",
        "    # Common configuration for all runs.\n",
        "    config = dnnlib.EasyDict(\n",
        "        train_dataset       = args.train_h5,            # Training set.\n",
        "        validation_dataset  = args.validation_set,      # Dataset used to monitor validation convergence during training.\n",
        "        validation_repeats  = validation_repeats,\n",
        "        num_channels        = 3,                        # RGB.\n",
        "        train_resolution    = 256,\n",
        "        minibatch_size      = 4,\n",
        "        learning_rate       = 3e-4,\n",
        "        config_name         = config_name,\n",
        "        dataset_dir         = args.dataset_dir\n",
        "    )\n",
        "\n",
        "    selected_config = config_map[config_name]\n",
        "    config.update(**selected_config)\n",
        "    if args.eval is not None:\n",
        "        config['eval_network'] = args.eval\n",
        "    del config['eval_id']\n",
        "\n",
        "    #----------------------------------------------------------------------------\n",
        "\n",
        "    # Execute.\n",
        "    sc.run_desc = 'eval' if config.get('eval_network') else 'train'\n",
        "\n",
        "    # Decorate run_desc.\n",
        "    sc.run_desc += '-ilsvrc'\n",
        "    if config.get('prune_dataset'): sc.run_desc += '_%d' % config.prune_dataset\n",
        "    sc.run_desc += '-%s' % config.validation_dataset\n",
        "    sc.run_desc += '-%dc' % config.num_channels\n",
        "    sc.run_desc += '-%s' % config.noise_style\n",
        "    if config.minibatch_size != 4:      sc.run_desc += '-mb%d' % config.minibatch_size\n",
        "    if config.learning_rate != 3e-4:    sc.run_desc += '-lr%g' % config.learning_rate\n",
        "    if config.num_iter >= 1000000:\n",
        "        sc.run_desc += '-iter%dm' % (config.num_iter // 1000000)\n",
        "    elif config.num_iter >= 1000:\n",
        "        sc.run_desc += '-iter%dk' % (config.num_iter // 1000)\n",
        "    else:\n",
        "        sc.run_desc += '-iter%d' % config.num_iter\n",
        "    sc.run_desc += '-%s' % config.pipeline\n",
        "    if config.get('diagonal_covariance'): sc.run_desc += 'Diag'\n",
        "    if config.pipeline == 'blindspot':\n",
        "        sc.run_desc += '-%s' % config.noise_params\n",
        "    if config.train_resolution != 256:  sc.run_desc += '-res%d' % config.train_resolution\n",
        "\n",
        "    if config.get('eval_network'): sc.run_desc += '-EVAL_%s' % config_name\n",
        "    if config.get('eval_network'): sc.run_dir_root += '/_eval'\n",
        "\n",
        "    # Submit.\n",
        "    submit.submit_run(sc, 'selfsupervised_denoising.train', **config)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84c249347f8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtflib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdnnlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtflib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtfutil\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dnnlib'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}